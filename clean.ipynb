{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "880bd647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'postings.csv'\n",
    "df_or = pd.read_csv(file_path)\n",
    "\n",
    "df = df_or[['job_id','company_name', 'title','description','skills_desc','normalized_salary','formatted_experience_level','formatted_work_type','remote_allowed','posting_domain','location','listed_time','zip_code']]\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2abb4f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_33860\\2034337699.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .replace({\n"
     ]
    }
   ],
   "source": [
    "df = df.fillna({\n",
    "    'company_name': 'Unknown',\n",
    "    'description': '',\n",
    "    'skills_desc': '',\n",
    "    'normalized_salary': 0,\n",
    "    'remote_allowed': False,\n",
    "    'zip_code': 'Unknown'\n",
    "})\n",
    "df['remote_allowed'] = (\n",
    "    df['remote_allowed']\n",
    "    .replace({\n",
    "        1.0: True, 0.0: False,\n",
    "        1: True, 0: False,\n",
    "        '1.0': True, '0.0': False,\n",
    "        '1': True, '0': False,\n",
    "        'true': True, 'false': False,\n",
    "        'True': True, 'False': False\n",
    "    })\n",
    "    .astype('boolean'))\n",
    "df['zip_code'] = df['zip_code'].astype(str).str.strip().fillna('Unknown')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ff2175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base dataset (for NLP + analysis)\n",
    "df_base = df.dropna(subset=['job_id','title','description'])\n",
    "df_base = df_base.drop_duplicates(subset='job_id')\n",
    "\n",
    "# Salary dataset (for prediction)\n",
    "df_salary = df_base.copy()\n",
    "df_salary['normalized_salary'] = pd.to_numeric(df_salary['normalized_salary'], errors='coerce')\n",
    "df_salary = df_salary.dropna(subset=['normalized_salary'])\n",
    "df_salary = df_salary[df_salary['normalized_salary'] > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c0b1cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.to_csv('cleaned_postings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "279000fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKILL_LIST = [\n",
    "    \n",
    "    # Programming Languages\n",
    "    \"python\", \"java\", \"c\", \"c++\", \"c#\", \"javascript\", \"typescript\",\n",
    "    \"go\", \"golang\", \"rust\", \"scala\", \"kotlin\", \"swift\", \"ruby\", \"php\",\n",
    "    \"r\", \"matlab\", \"bash\", \"shell\", \"powershell\", \"sql\",\n",
    "\n",
    "    # Data Analysis / Scientific Python\n",
    "    \"pandas\", \"numpy\", \"scipy\", \"statsmodels\", \"jupyter\", \"jupyter notebook\",\n",
    "    \"matplotlib\", \"plotly\",\n",
    "\n",
    "    # Databases & Storage\n",
    "    \"postgresql\", \"postgres\", \"mysql\", \"mariadb\", \"sqlite\",\n",
    "    \"mssql\", \"sql server\", \"oracle\",\n",
    "    \"mongodb\", \"cassandra\", \"dynamodb\", \"cosmos db\",\n",
    "    \"redis\", \"elasticsearch\", \"opensearch\", \"neo4j\",\n",
    "    \"snowflake\", \"redshift\", \"bigquery\", \"synapse\", \"teradata\",\n",
    "    \"s3\", \"adls\", \"gcs\",\n",
    "\n",
    "    # File formats / table formats\n",
    "    \"parquet\", \"avro\", \"orc\",\n",
    "    \"delta lake\", \"delta\", \"iceberg\", \"hudi\",\n",
    "\n",
    "    # Big Data / Streaming / Messaging\n",
    "    \"apache spark\", \"spark\", \"pyspark\",\n",
    "    \"hadoop\", \"hdfs\", \"yarn\",\n",
    "    \"hive\", \"presto\", \"trino\",\n",
    "    \"kafka\", \"kinesis\", \"pubsub\", \"google pubsub\", \"pub/sub\",\n",
    "    \"flink\", \"spark streaming\", \"storm\",\n",
    "    \"rabbitmq\", \"sqs\", \"sns\",\n",
    "\n",
    "    # Orchestration / ETL / ELT\n",
    "    \"etl\", \"elt\", \"data pipeline\", \"data pipelines\",\n",
    "    \"airflow\", \"dagster\", \"prefect\", \"luigi\",\n",
    "    \"dbt\", \"fivetran\", \"stitch\",\n",
    "    \"informatica\", \"talend\", \"ssis\",\n",
    "\n",
    "    # Cloud Platforms & Services\n",
    "    \"aws\", \"amazon web services\", \"azure\", \"gcp\", \"google cloud\",\n",
    "    # AWS specifics\n",
    "    \"lambda\", \"api gateway\", \"ecs\", \"ecr\", \"eks\", \"fargate\",\n",
    "    \"sagemaker\", \"cloudwatch\", \"cloudformation\", \"athena\", \"glue\",\n",
    "    # Azure specifics\n",
    "    \"azure functions\", \"aks\", \"acr\", \"azure devops\", \"data factory\",\n",
    "    # GCP specifics\n",
    "    \"cloud functions\", \"cloud run\", \"gke\", \"bigtable\", \"dataflow\",\n",
    "\n",
    "\n",
    "    # Containers / Infra / DevOps\n",
    "    \"docker\", \"kubernetes\", \"helm\",\n",
    "    \"terraform\", \"pulumi\", \"ansible\",\n",
    "    \"jenkins\", \"github actions\", \"gitlab ci\", \"circleci\", \"ci/cd\",\n",
    "    \"nginx\", \"apache\",\n",
    "\n",
    "    # Backend / APIs\n",
    "    \"rest\", \"rest api\", \"graphql\", \"grpc\", \"soap\",\n",
    "    \"fastapi\", \"flask\", \"django\",\n",
    "    \"node.js\", \"nodejs\", \"express\",\n",
    "    \"spring\", \"spring boot\", \".net\", \"asp.net\", \"asp.net core\",\n",
    "\n",
    "    # Frontend (common in full-stack postings)\n",
    "    \"react\", \"next.js\", \"nextjs\", \"vue\", \"angular\",\n",
    "    \"html\", \"css\", \"sass\", \"tailwind\", \"bootstrap\",\n",
    "\n",
    "    # Machine Learning / AI Fundamentals\n",
    "    \"machine learning\", \"deep learning\", \"artificial intelligence\",\n",
    "    \"statistics\", \"time series\", \"forecasting\",\n",
    "    \"natural language processing\", \"nlp\",\n",
    "    \"computer vision\", \"recommendation systems\", \"recommender systems\",\n",
    "    \"a/b testing\", \"ab testing\", \"experiment design\",\n",
    "\n",
    "    # ML models / methods\n",
    "    \"linear regression\", \"logistic regression\",\n",
    "    \"random forest\", \"xgboost\", \"lightgbm\", \"catboost\",\n",
    "    \"svm\", \"naive bayes\", \"k-means\", \"clustering\",\n",
    "    \"neural networks\",\n",
    "\n",
    "    # ML / AI Frameworks\n",
    "    \"scikit-learn\", \"sklearn\",\n",
    "    \"tensorflow\", \"keras\", \"pytorch\",\n",
    "    \"xgboost\", \"lightgbm\",  # (kept here too because postings repeat)\n",
    "    \"hugging face\", \"transformers\",\n",
    "    \"spacy\", \"nltk\",\n",
    "    \"opencv\",\n",
    "    \"mlflow\", \"kubeflow\",\n",
    "\n",
    "    # GenAI / LLM (very common now)\n",
    "    \"llm\", \"large language model\", \"generative ai\", \"genai\",\n",
    "    \"prompt engineering\", \"rag\", \"retrieval augmented generation\",\n",
    "    \"vector database\", \"vector db\", \"embeddings\",\n",
    "    \"langchain\", \"llamaindex\",\n",
    "    \"openai\", \"azure openai\",\n",
    "    \"pinecone\", \"weaviate\", \"milvus\", \"faiss\", \"chromadb\", \"chroma\",\n",
    "\n",
    "    # MLOps / Deployment / Serving\n",
    "    \"mlops\", \"model deployment\", \"model serving\", \"model monitoring\",\n",
    "    \"feature store\",\n",
    "    \"seldon\", \"bentoml\", \"ray serve\",\n",
    "    \"onnx\",\n",
    "\n",
    "    # Observability / Monitoring / Logging\n",
    "    \"prometheus\", \"grafana\",\n",
    "    \"datadog\", \"new relic\",\n",
    "    \"elk\", \"splunk\",\n",
    "\n",
    "    # Security (often in job reqs)\n",
    "    \"oauth\", \"oauth2\", \"openid connect\", \"jwt\",\n",
    "    \"iam\", \"rbac\",\n",
    "    \"encryption\", \"tls\", \"ssl\",\n",
    "\n",
    "    # Testing / Quality\n",
    "    \"unit testing\", \"integration testing\", \"e2e testing\",\n",
    "    \"pytest\", \"unittest\", \"junit\",\n",
    "    \"selenium\", \"cypress\",\n",
    "\n",
    "    # Version Control / Collaboration\n",
    "    \"git\", \"github\", \"gitlab\", \"bitbucket\",\n",
    "    \"agile\", \"scrum\", \"jira\", \"confluence\",\n",
    "\n",
    "    # BI / Analytics Tools\n",
    "    \"excel\", \"power bi\", \"tableau\", \"looker\", \"metabase\", \"superset\",\n",
    "\n",
    "    # Data Modeling / Warehousing Concepts\n",
    "    \"data modeling\", \"dimensional modeling\", \"star schema\", \"snowflake schema\",\n",
    "    \"data warehousing\", \"data lake\", \"lakehouse\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "491f22fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp.make_doc(name) for name in SKILL_LIST]\n",
    "matcher.add(\"Skills\", patterns)\n",
    "desc = df_base['description'].fillna(\"\").astype(str)\n",
    "skills = df_base['skills_desc'].fillna(\"\").astype(str)\n",
    "\n",
    "desc_skill = (desc + \" \" + skills).str.strip().tolist()\n",
    "batch = 256\n",
    "result = []\n",
    "for i, doc in enumerate(nlp.pipe(desc_skill,batch_size=batch)):\n",
    "    matches = matcher(doc)\n",
    "    skills_found = set()\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        skills_found.add(span.text.lower())\n",
    "    result.append(\"|\".join(sorted(skills_found)))\n",
    "\n",
    "df_base['extracted_skills'] = result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2eae95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base['zip_code'] = df_base['zip_code'].astype(str).str.strip().fillna('Unknown')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c58b0b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.to_csv('cleaned_postings_with_skills.csv', index=False)\n",
    "df_base. to_parquet('cleaned_postings.parquet',engine='pyarrow', compression='snappy',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
